import ast
import json
import os.path

import intake_esm
import pandas as pd
import pytest
from dask.distributed import Client
from gen_test_input import gen_test_input
from packaging import version

from esm_catalog_utils import case_metadata_to_esm_datastore, date_parser


def dict_cmp(d1, d2, ignore_keys=None):
    """compare dictionaries, ignoring keys in ignore_keys"""

    d1_subset = {key: d1[key] for key in d1 if key not in ignore_keys}
    d2_subset = {key: d2[key] for key in d2 if key not in ignore_keys}
    return d1_subset == d2_subset


def read_catalog_csv(path):
    """read csv portion of catalog"""
    return pd.read_csv(
        path,
        converters={
            "variable": ast.literal_eval,
            "date_start": date_parser,
            "date_end": date_parser,
        },
    )


@pytest.mark.parametrize("parallel", [False, True])
@pytest.mark.parametrize("incremental_catalog_gen", [False, True])
# @pytest.mark.parametrize("parallel", [False])
# @pytest.mark.parametrize("incremental_catalog_gen", [False])
def test_gen_esmcol_files(parallel, incremental_catalog_gen):
    repo_root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
    generated_dir = os.path.join(repo_root, "tests", "generated")
    baseline_dir = os.path.join(repo_root, "tests", "baselines")

    cases_metadata = gen_test_input(generated_dir)

    if parallel:
        # avoid threads because of
        # https://github.com/Unidata/netcdf4-python/issues/1192
        client = Client(n_workers=2, threads_per_worker=1)

    for case_metadata in cases_metadata:
        case = case_metadata["case"]
        file_type = os.path.basename(case_metadata["output_dirs"][0])
        print(f"case={case}, file_type={file_type}")

        # generate esm_datastore object from case_metadata
        if incremental_catalog_gen:
            for dir_ind, output_dir in enumerate(case_metadata["output_dirs"]):
                case_metadata_subset = {
                    "case": case_metadata["case"],
                    "output_dirs": [output_dir],
                }
                if dir_ind == 0:
                    esm_datastore = case_metadata_to_esm_datastore(
                        case_metadata_subset, use_dask=parallel
                    )
                else:
                    esm_datastore = case_metadata_to_esm_datastore(
                        case_metadata_subset,
                        esm_datastore_in=esm_datastore,
                        use_dask=parallel,
                    )
        else:
            esm_datastore = case_metadata_to_esm_datastore(
                case_metadata, use_dask=parallel
            )

        # write esm_datastore object to disk
        esm_datastore.serialize(
            name=f"{case}_{file_type}", directory=generated_dir, catalog_type="file"
        )

        # compare generated csv file to baseline
        # after replacing REPO_ROOT with repo_root in path
        # ignore size key, as file size generated by gen_test_input varies depending
        #     on versions in stack
        if version.Version(intake_esm.__version__) < version.Version("2022.9.18"):
            fname = f"{case}_{file_type}.csv.gz"
        else:
            fname = f"{case}_{file_type}.csv"
        generated = read_catalog_csv(os.path.join(generated_dir, fname))
        if version.Version(intake_esm.__version__) < version.Version("2022.9.18"):
            fname = f"pre_2022.9.18_{fname}"
        baseline = read_catalog_csv(os.path.join(baseline_dir, fname))
        assert len(baseline.columns) == len(generated.columns)
        assert (baseline.columns == generated.columns).all()
        for key in baseline:
            if key == "size":
                continue
            elif key != "path":
                assert (baseline[key] == generated[key]).all()
            else:
                baseline_replace = pd.Series(
                    [foo.replace("REPO_ROOT", repo_root) for foo in baseline[key]]
                )
                assert (baseline_replace == generated[key]).all()

        # compare generated json file to baseline
        # after replacing REPO_ROOT with repo_root in catalog_file
        # ignore last_updated key
        fname = f"{case}_{file_type}.json"
        with open(os.path.join(generated_dir, fname), mode="r") as fptr:
            generated = json.load(fptr)
        if version.Version(intake_esm.__version__) < version.Version("2022.9.18"):
            fname = f"pre_2022.9.18_{fname}"
        with open(os.path.join(baseline_dir, fname), mode="r") as fptr:
            baseline = json.load(fptr)

        for key in ["catalog_file"]:
            baseline[key] = baseline[key].replace("REPO_ROOT", repo_root)
        assert dict_cmp(baseline, generated, ignore_keys=["last_updated"])

    if parallel:
        client.close()
